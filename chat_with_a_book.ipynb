{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ywe1X5kcOqjX"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'detectron2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m UnstructuredPDFLoader\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext_splitter\u001b[39;00m \u001b[39mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdetectron2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Pinecone\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mopenai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "86L0S0zVRZOF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with another strategy.\n",
            "Falling back to partitioning with ocr_only.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loader \u001b[39m=\u001b[39m UnstructuredPDFLoader(\u001b[39m\"\u001b[39m\u001b[39mHandBook_DS.pdf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mYou have \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m document(s) in your data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpage_content)\u001b[39m}\u001b[39;00m\u001b[39m characters in your document\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/site-packages/langchain/document_loaders/unstructured.py:70\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     69\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     71\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     72\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/site-packages/langchain/document_loaders/pdf.py:36\u001b[0m, in \u001b[0;36mUnstructuredPDFLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m     34\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpdf\u001b[39;00m \u001b[39mimport\u001b[39;00m partition_pdf\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_pdf(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/site-packages/unstructured/partition/pdf.py:75\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[0;34m(filename, file, url, template, token, include_page_breaks, strategy, infer_table_structure, encoding, ocr_languages)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parses a pdf document into a list of interpreted elements.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m    to isntall the appropriate Tesseract language pack.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m exactly_one(filename\u001b[39m=\u001b[39mfilename, file\u001b[39m=\u001b[39mfile)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m partition_pdf_or_image(\n\u001b[1;32m     76\u001b[0m     filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m     77\u001b[0m     file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m     78\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m     79\u001b[0m     template\u001b[39m=\u001b[39;49mtemplate,\n\u001b[1;32m     80\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m     81\u001b[0m     include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[1;32m     82\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     83\u001b[0m     infer_table_structure\u001b[39m=\u001b[39;49minfer_table_structure,\n\u001b[1;32m     84\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m     85\u001b[0m     ocr_languages\u001b[39m=\u001b[39;49mocr_languages,\n\u001b[1;32m     86\u001b[0m )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/site-packages/unstructured/partition/pdf.py:146\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[0;34m(filename, file, url, template, token, is_image, include_page_breaks, strategy, infer_table_structure, encoding, ocr_languages)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39melif\u001b[39;00m strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mocr_only\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    144\u001b[0m         \u001b[39m# NOTE(robinson): Catches file conversion warnings when running with PDFs\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m--> 146\u001b[0m             \u001b[39mreturn\u001b[39;00m _partition_pdf_or_image_with_ocr(\n\u001b[1;32m    147\u001b[0m                 filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    148\u001b[0m                 file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m    149\u001b[0m                 include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[1;32m    150\u001b[0m                 ocr_languages\u001b[39m=\u001b[39;49mocr_languages,\n\u001b[1;32m    151\u001b[0m                 is_image\u001b[39m=\u001b[39;49mis_image,\n\u001b[1;32m    152\u001b[0m             )\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# NOTE(alan): Remove these lines after different models are handled by routing\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m template \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheckbox\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/site-packages/unstructured/partition/pdf.py:325\u001b[0m, in \u001b[0;36m_partition_pdf_or_image_with_ocr\u001b[0;34m(filename, file, include_page_breaks, ocr_languages, is_image)\u001b[0m\n\u001b[1;32m    323\u001b[0m     file\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     document \u001b[39m=\u001b[39m pdf2image\u001b[39m.\u001b[39;49mconvert_from_path(filename)\n\u001b[1;32m    327\u001b[0m \u001b[39mfor\u001b[39;00m i, image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(document):\n\u001b[1;32m    328\u001b[0m     metadata \u001b[39m=\u001b[39m ElementMetadata(filename\u001b[39m=\u001b[39mfilename, page_number\u001b[39m=\u001b[39mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/site-packages/pdf2image/pdf2image.py:250\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mfor\u001b[39;00m uid, proc \u001b[39min\u001b[39;00m processes:\n\u001b[1;32m    249\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         data, err \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39;49mcommunicate(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    251\u001b[0m     \u001b[39mexcept\u001b[39;00m TimeoutExpired:\n\u001b[1;32m    252\u001b[0m         proc\u001b[39m.\u001b[39mkill()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/subprocess.py:1207\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1207\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[1;32m   1208\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1209\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/gpts/lib/python3.11/subprocess.py:2095\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2093\u001b[0m             key\u001b[39m.\u001b[39mfileobj\u001b[39m.\u001b[39mclose()\n\u001b[1;32m   2094\u001b[0m \u001b[39melif\u001b[39;00m key\u001b[39m.\u001b[39mfileobj \u001b[39min\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr):\n\u001b[0;32m-> 2095\u001b[0m     data \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mread(key\u001b[39m.\u001b[39mfd, \u001b[39m32768\u001b[39m)\n\u001b[1;32m   2096\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m   2097\u001b[0m         selector\u001b[39m.\u001b[39munregister(key\u001b[39m.\u001b[39mfileobj)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loader = UnstructuredPDFLoader(\"HandBook_DS.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "print (f'You have {len(data)} document(s) in your data')\n",
        "print (f'There are {len(data[0].page_content)} characters in your document')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ss0wAVDgRjyy"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SLdPGCIjSMa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now you have 1884 documents\n"
          ]
        }
      ],
      "source": [
        "print (f'Now you have {len(texts)} documents')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PY-EzgxfSPyy"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = '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'\n",
        "PINECONE_API_KEY = '1b9217b3-617f-439c-99a0-a56a19ba307c'\n",
        "PINECONE_API_ENV = 'us-west1-gcp-free'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M7UUuC38S7bd"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O63vfodTTA0i"
      },
      "outputs": [],
      "source": [
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "index_name = \"limitsofgrowth\" # put in the name of your pinecone index here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ssy85NCusnY1"
      },
      "outputs": [],
      "source": [
        "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SKrlZeaus3qf"
      },
      "outputs": [],
      "source": [
        "query = \"Explain the no free lunch theorem\"\n",
        "docs = docsearch.similarity_search(query, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cGFflUwlvCQg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='There is even a mathematical theorem proving that this will always be the case: You cannot create a machine learning algorithm that can work for all possible problems. This proof is popularly called a “no free lunch theorem”.”! The implication of the theorem is basically that there is no free lunch in machine learning: If you change the inductive biases of your model to gain something, you will necessarily have to pay the price; you will at the same time lose something. What you will lose is exactly the opposite of your inductive bias. If you assume ReLu transfer function, you will make it more difficult to learn everything non-ReLu; if you assume sine waves, you will have difficulties with linear functions. And so on. Also, if you create a more elaborate model that combines sine waves and ReLu, you will still lose some- thing. You will reduce the capability to learn and hence, you will need more data due to the larger number of parameters. And so on. This is a game in which you can', metadata={}),\n",
              " Document(page_content='© 2021 Carl Hanser Verlag. All rights reserved. No unauthorized disclosure or reproduction; licensed to purchaser only.\\n\\nlevel Intelligence\\n\\nSimilarly, we are still yet to find an AGI paper that acknowledges the existence of the no-free-lunch-theorem and designs its research efforts around this. The human brain must be paying some price for the intelligence it gains. So must any Al approach that mimics this intelligence.', metadata={}),\n",
              " Document(page_content='And this brings us to our final and perhaps ultimate thought about how to defend ourselves against the no-free-lunch-theorem. We have to use our minds to do great engineering. We must add pieces of algorithms that jump out of the world of the other algorithms that we are already using. In the end we will have, as I already stated multiple times, an elaborate machine containing multiple pieces. Some of them will rely on machine learning. Others will be best described as GOFAI. You need both and you will have both. Finding a good solu-\\n\\n4 The reason my comparisons with Lego bricks work so well in this text is because | am not merely making a metaphor here. Lego is as much a model of the world as is any other mathematical model. | am hence comparing two equals: Lego base model to deep learning based model.', metadata={}),\n",
              " Document(page_content='So, what can we do to protect ourselves from this trap? How do we suppress our seductive intuition? First, we must stay aware, regularly reminding ourselves that there are no free lunches in machine learning. A good way to do this is not to ask questions like “Which pow- erful idea can I come up with that will solve all the problems?”. Instead, we should ask something along the lines of “Which trade-off can I make; what am I ready to give away and what can I hope to obtain in return?” The latter type of questions may guide you to make more realistic design decisions for your AI architectures.\\n\\n°° https://de. wikipedia. org/wiki/Mersenne\\n\\n\\n\\nTwister\\n\\n2! https://en.wikipedia.org/wiki/No_free_lunch_theorem, https://machinelearningmastery.com/no\\n\\n\\n\\nfree\\n\\n\\n\\nlunch\\n\\n\\n\\ntheo\\n\\n\\n\\nrem\\n\\n\\n\\nfor\\n\\n\\n\\nmachine\\n\\n\\n\\nlearning/\\n\\n© 2021 Carl Hanser Verlag. All rights reserved. No unauthorized disclosure or reproduction; licensed to purchaser only.\\n\\n266\\n\\n9 Building Great Artificial Intelligence', metadata={}),\n",
              " Document(page_content='A common trap is in the published work. When we read papers on fresh new algorithms, the papers often present only one side of the picture. They tell you how well the algorithm works on their data. But what they typically fail to do is to explain which price they had to pay for their lunch. Usually, this part is entirely skipped, and one is left with the impression that the new algorithm only brings benefits and there are no disadvantages to it. As a min- imum, we have to be wary of that fact. We have to read between the lines and detect by ourselves the price for the lunch being offered. We have to then know whether we are will- ing to pay this price - or whether we are at all in the position to make the payment. It is better to ask this question immediately, while reading the paper, than discovering the same answer the hard way - after several months of effort with trial and error in unsuccessful attempts to make the method work for you (although, sometimes there is no other option', metadata={})]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lk34Tkm_vvx5"
      },
      "source": [
        "### Query those docs to get an answer back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T6LqAGHDvDBK"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qi3iirPzv12Z"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pcQVofRKv4Fs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The no free lunch theorem states that no machine learning algorithm can work for all possible problems. This means that if you change the inductive biases of your model to gain something, you will necessarily have to pay the price; you will at the same time lose something. This is because the inductive bias you assume will make it more difficult to learn the opposite of what you assumed. For example, if you assume a ReLu transfer function, you will make it more difficult to learn everything non-ReLu; if you assume sine waves, you will have difficulties with linear functions. This means that you will need more data due to the larger number of parameters. To protect ourselves from this trap, we must use our minds to do great engineering and add pieces of algorithms that jump out of the world of the other algorithms that we are already using. This way, we can create an elaborate machine containing multiple pieces, some of which rely on machine learning and others which are best described as GOFAI.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Explain the no free lunch theorem in great detail!\"\n",
        "docs = docsearch.similarity_search(query,k=5)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZVEgrwCkucm"
      },
      "outputs": [],
      "source": [
        "knowledge_book = {}\n",
        "\n",
        "while True: \n",
        "  query = input(\"What do you want to know?\\n\")\n",
        "  docs = docsearch.similarity_search(query,k=5)\n",
        "  output = chain.run(input_documents=docs, question=query)\n",
        "  print(output)\n",
        "  follow_up = input(\"Do you want to save it? (Y/N)?\")\n",
        "  if follow_up == \"Y\": \n",
        "    knowledge_book[query] = output.lstrip() \n",
        "  else: \n",
        "    continue\n",
        "\n",
        "  next = input(\"Do you have another question? (Y/N)\")\n",
        "  if next == \"Y\": \n",
        "    query = input(\"Tell me what you want to know\\n\")\n",
        "    docs = docsearch.similarity_search(query,k=5)\n",
        "    output = chain.run(input_documents=docs, question=query)\n",
        "    print(output)\n",
        "    follow_up = input(\"Do you want to save it? (Y/N)?\")\n",
        "    if follow_up == \"Y\": \n",
        "      knowledge_book[query] = output.lstrip()\n",
        "    else: \n",
        "      further = input(\"Continue? (Y/N\")\n",
        "      if further == \"Y\": \n",
        "        continue\n",
        "  \n",
        "  else: \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx8rJBAa_Oep"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import openai\n",
        "\n",
        "# Set up Tweepy with your credentials\n",
        "consumer_key = \"jgrcubBJEfEejhl0MjGvOE6ZV\"\n",
        "consumer_secret = \"u3in1Hz9IO2FAKN3fuqHKefJzesdNhXHFuRgiRXG7vtyTTvfph\"\n",
        "access_token = \"772831327406161920-dI6Pchv21osVgXWf7JHA4u7dZnrQtIu\"\n",
        "access_token_secret = \"0KdxCAUdIwnssMGswDsRNGGRB2aBRFp0MdxlADpkKkFNr\"\n",
        "\n",
        "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Iterate over the dictionary and generate hashtags with OpenAI\n",
        "for key, value in knowledge_book.items():\n",
        "\n",
        "    # Construct the tweet\n",
        "    tweet = f\"#QueryTheBook: The Limits of Growth by the Club of Rome: {key} {value}\"\n",
        "\n",
        "    # Check if the tweet is too long\n",
        "    if len(tweet) > 280:\n",
        "        # Split the tweet into multiple tweets with proper length\n",
        "        new_tweet = tweet.split(\"?\")[0] + \"?\"\n",
        "        comment = tweet.split(\"?\")[1]\n",
        "\n",
        "        if len(comment) > 280: \n",
        "          #split the comment further\n",
        "          split_string = comment.split()\n",
        "          print(split_string)\n",
        "          comment =' '.join([str(item) for item in split_string[:round(len(split_string)/2)]])\n",
        "          comment1 =' '.join([str(item) for item in split_string[round(len(split_string)/2):]])\n",
        "\n",
        "          tweet = api.update_status(new_tweet)  \n",
        "          comment = api.update_status(comment, in_reply_to_status_id=tweet.id, auto_populate_reply_metadata=True)\n",
        "          comment1 = api.update_status(comment1, in_reply_to_status_id=comment.id, auto_populate_reply_metadata=True)\n",
        "\n",
        "        else: \n",
        "          tweet = api.update_status(new_tweet)  \n",
        "          comment = api.update_status(comment, in_reply_to_status_id=tweet.id, auto_populate_reply_metadata=True)\n",
        "\n",
        "    else:\n",
        "        # Post the tweet\n",
        "        api.update_status(tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKAaAavb4H0j"
      },
      "outputs": [],
      "source": [
        "knowledge_book"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpqHbSrQ4I16"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
